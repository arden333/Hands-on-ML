{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 머신러닝이란?\n",
    "- 데이터에서부터 학습하도록 컴퓨터를 프로그래밍하는 과학\n",
    "\n",
    "### 2. 머신러닝 왜 필요?\n",
    "- 데이터마이닝: 머신러닝 기술을 적용해 대용량의 데애터 패턴 파악 가능\n",
    "- 뛰어난 분야: 많은 수동 조정과 규칙이 필요한 문제, 해결방법이 없는 복잡한 문제, 유동적 환경\n",
    "\n",
    "### 3. 종류\n",
    "#### 3-1. 지도학습 & 비지도학습\n",
    "- by 학습하는 동안의 감독 형태, 정보량\n",
    "1. 지도학습: 훈련 데이터에 레이블이 포함되어 있음\n",
    "    - 알고리즘: k근접이웃, 선형회귀, 로지스틱회귀, 서포트벡터머신(SVM), 결정트리, 랜덤포레스트, 신경망\n",
    "\n",
    "2. 비지도학습: 훈련 데이터에 레이블이 포함X\n",
    "    - 알고리즘\n",
    "        - 군집: k-평균, DBSCAN, 계층군집분석(HCA)\n",
    "        - 이상치: 특이치 탐지, 원-클래스, isolation forest\n",
    "        - 시각화, 차원 축소: PCA, 커널PCA, locally-linear embeding(상관관계 있는 특성을 하나로), t-SNE\n",
    "        - 연관규칙학습: Apriori, Eclat\n",
    "\n",
    "3. 준지도학습: 훈련 데이터 일부만 레이블 있음, 지도&비지도학습의 조합\n",
    "\n",
    "4. 강화학습: 학습시스템(에이전트) - 보상(reward), 벌점(penalty)로 학습\n",
    "\n",
    "#### 3-2. 배치학습 & 온라인학습\n",
    "- by 실시간 점진적 학습 여부\n",
    "1. 배치학습: 점진적 학습 불가, 가용한 데이터를 모두 사용해야 함. 시간과 자원 소모 많음\n",
    "2. 온라인학습: 미니배치로 주입해 훈련, 학습단계 빠르고 비용 적음\n",
    "    - 학습률: 변화하는 데이터에 얼마나 빠르게 적응할 것인지 결정. 낮으면 시스템 관성 커져 느리게 학습, 빠르면 새로운 데이터만 반영\n",
    "\n",
    "#### 3-3. 사례기반 & 모델기반 학습 \n",
    "- by 어떻게 일반화되는가\n",
    "1. 사례기반: 시스템이 훈련 샘플을 기억함으로서 학습. 유사도 측정을 통해 새 데이터와 학습한 샘플 비교\n",
    "2. 모델기반: 모델을 만들어 예측에 사용\n",
    "    - 모델선택, 모델 파라미터, 효용함수(모델이 얼마나 좋은지), 비용함수(모델이 얼마나 나쁜지. 예측과 훈련 데이터 간 거리 재는 함수. 최소화하는 것이 목표), 훈련(최적의 파라미터 찾음)\n",
    "    - 데이터 분석 → 모델선택 → 훈련데이터로 모델 훈련 → 새로운 데이터에 모델 적용해 예측\n",
    "\n",
    "### 4. ML 주요 도전과제\n",
    "#### 4-1. 나쁜 데이터\n",
    "1. 충분하지 않은 훈련 데이터\n",
    "2. 대표성 없는 훈련 데이터\n",
    "    - 샘플링 잡음: 샘플이 적은 경우\n",
    "    - 샘플링 편향: 샘플은 크나 표본 추출이 잘못된 경우\n",
    "3. 낮은 품질 데이터: 이상치, 일부 특성 누락, 오류\n",
    "4. 관련 없는 특성 → 특성공학\n",
    "    - 특성 선택(selection): 훈련에 가장 유용한 특성 선택\n",
    "    - 특성 추출(extraction): 특성 결합해 유용한 특성 생성, 차원 축소 알고리즘 활용\n",
    "\n",
    "#### 4-2. 나쁜 알고리즘\n",
    "1. 과대적합: 모델이 훈련 데이터에 잘 맞으나 일반성 떨어짐(모델 복잡)\n",
    "    (sol) 파라미터가 적은 모델 선택, 훈련데이터의 특성 줄이기, 모델에 제약 가해 단순화(규제), 훈련 데이터 많이, 훈련 데이터의 잡음 줄이기(이상치, 데이터 오류)\n",
    "\n",
    "    - 규제(regularization): 모델을 단순하게 하고 과대적합의 위험 감소시킴   \n",
    "      (ex) 선형모델은 y = b0 + b1 * x. b0, b1이라는 자유도를 알고리즘에 부여. b1=0이 되도록 강제하면 한 개의 자유도만 남아 데이터에 딱 맞게 맞춰지기 힘듦\n",
    "    - 하이퍼파라미터: 알고리즘이 아닌 학습 알고리즘의 파라미터. 학습하는 동안 적용할 규제의 양 결정\n",
    "\n",
    "2. 과소적합: 모델이 너무 단순해서 데이터의 구조를 제대로 학습하지 못함\n",
    "    (sol): 모델 파라미터가 많은 모델을 선택, 특성공학, 모델의 제약 줄이기(규제 하이퍼파라미터 감소)\n",
    "\n",
    "### 테스트와 검증\n",
    "- 훈련 데이터를 train set, test set으로 나눔\n",
    "- 일반화 오차(generalization error): 새로운 샘플에 대한 오류 비율 - estimation(오차에 대한 추정값)   \n",
    "    → 훈련 오차 작고 일반화 오차 크면 과대적합\n",
    "- 하이퍼파라미터 튜닝과 모델선택\n",
    "    홀드아웃검증(holdout validation): 검증세트가 너무 작거나 크면 문제 발생 → 교차검증(cross-validation)\n",
    "- 데이터 불일치: 실제를 완벽히 대표하지 못하는 데이터 → 훈련세트에서 훈련 후 훈련-개발 세트(train-dev set)에서 평가\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
